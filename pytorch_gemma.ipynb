{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30cd1b4e",
   "metadata": {},
   "source": [
    "# Pytorch version to use gemma model locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cd7345",
   "metadata": {},
   "source": [
    "Copyright 2024 Paul T. Miller > For Academic Use\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a67402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from pytorch_gemma import GemmaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c9387d",
   "metadata": {},
   "source": [
    "# Modify the parameters as desired\n",
    "### output_size indicates the max you allow for model output.\n",
    "### version options are '2b', '2b-it', '7b', 7b-it'. 'it' indicates a pre-trained model. \n",
    "### Machine - use 'cuda' if you have cuda installed with your Nvidia GPU, otherwise use 'cpu'\n",
    "### Download the desired model from https://www.kaggle.com/models/google/gemma/frameworks/pyTorch\n",
    "### Extract contents from the downloaded archive.tar.gz and put the files under the \"model/<version>/\" folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5b3393",
   "metadata": {},
   "source": [
    "### In Jupyter they look something like:\n",
    "<img src=\"models/2b_it_pic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9511d0",
   "metadata": {},
   "source": [
    "### In file viewer they look something like:\n",
    "<img src=\"models/files.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d68e65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cuda device is available on this system. Try to use it below for machine.\n"
     ]
    }
   ],
   "source": [
    "## Originally tested on Nvidia RTX2060 with 6GB VRAM on an Asus laptop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"The {device} device is available on this system. Try to use it below for machine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9a850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "model = GemmaModel(output_len=250, version='2b-it', machine='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188c9ff2",
   "metadata": {},
   "source": [
    "# Run the following and enter your question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e451cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.output_len = 1000 ## we can modify on the fly if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7252af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can manually set the request\n",
    "## text = \"Explain Einstein's general theory of relativity\"\n",
    "## print(model.get_response(text))\n",
    "## or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b97bbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello HUMAN!!!\n",
      "What is your question? (q to quit): Write a poem about a pink helicopter, which is standard issue in the Marine Corp.\n",
      "\n",
      "\n",
      "The pink helicopter, oh so bright,\n",
      "A beacon in the cerulean light.\n",
      "The Marine Corp's symbol, a daring deed,\n",
      "A pink speck against the endless field.\n",
      "\n",
      "Its blades flutter in the balmy breeze,\n",
      "An aerial warrior, a graceful tease.\n",
      "The rotor spun, a symbol of pride,\n",
      "As it dances through the sky so wide.\n",
      "\n",
      "A testament to courage and might,\n",
      "A chopper that evokes pure delight.\n",
      "The pink helicopter, a symbol strong,\n",
      "A legend that shall forever throng.\n",
      "What is your question? (q to quit): q\n",
      "You entered q : exiting now!\n"
     ]
    }
   ],
   "source": [
    "## use looped input\n",
    "text = model.get_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5768592c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75630b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

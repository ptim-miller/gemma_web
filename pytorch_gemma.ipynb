{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30cd1b4e",
   "metadata": {},
   "source": [
    "# Pytorch version to use gemma model locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cd7345",
   "metadata": {},
   "source": [
    "Copyright 2024 Paul T. Miller > For Academic Use\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a47288",
   "metadata": {},
   "source": [
    "# Get started\n",
    "## Install the latest NVIDIA system driver https://www.nvidia.com/download/index.aspx\n",
    "#### If you wish to add cuda and cudnn to your system:\n",
    "##### https://developer.nvidia.com/cuda-downloads\n",
    "##### https://developer.nvidia.com/cudnn-downloads\n",
    "  \n",
    "## Install torch with cuda support (https://pytorch.org/get-started/locally/)\n",
    "For Windows: \n",
    "+ pip3 install torch torchvision torchaudio --index-url https[]()://download.pytorch.org/whl/cu121\n",
    "+ pip3 install immutabledict sentencepiece Flask\n",
    "  \n",
    "For Linux: \n",
    "+ pip3 install torch torchvision torchaudio\n",
    "+ pip3 install immutabledict sentencepiece Flask\n",
    "  \n",
    "Or install using Conda with: \n",
    "+ conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a67402",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "os_type = platform.system()\n",
    "if os_type == 'Linux':\n",
    "    !pip3 install torch torchvision torchaudio\n",
    "else:\n",
    "    !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install immutabledict sentencepiece Flask\n",
    "import torch \n",
    "from pytorch_gemma import GemmaModel\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d43824e",
   "metadata": {},
   "source": [
    "## Add the large language model to the project\n",
    "+ Version options are '2b', '2b-it', '7b', 7b-it'. The 'it' in name indicates an instruction-tuned model.   \n",
    "+ Download the desired model from https://www.kaggle.com/models/google/gemma/frameworks/pyTorch. \n",
    "<img src=\"models/model_download.png\">\n",
    "+ <b>Recommend starting with 2b-it<b>. (Worked on a simple laptop with RTX2060 6GB VRAM).  \n",
    "+ Extract contents from the downloaded archive.tar.gz and put the files under the 'model/\\<version\\>/' folder.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5b3393",
   "metadata": {},
   "source": [
    "### In Jupyter they look something like:\n",
    "<img src=\"models/2b_it_pic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9511d0",
   "metadata": {},
   "source": [
    "### In file viewer they look something like:\n",
    "<img src=\"models/files.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afb2343",
   "metadata": {},
   "source": [
    "## Modify the parameters in the code as desired\n",
    "+ The output_size indicates the max allowed for model output. \n",
    "+ Machine - use 'cuda' if you have cuda installed with an Nvidia GPU, otherwise use 'cpu'.\n",
    "+ Use of 'cpu' is not recommended and will be slow. Drop output_size down if using 'cpu'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Originally tested on Nvidia RTX2060 with 6GB VRAM on an Asus laptop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"The {device} device is available on this system. Try to use it below for machine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "model = GemmaModel(output_len=250, version='2b-it', machine=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188c9ff2",
   "metadata": {},
   "source": [
    "# Run the following and enter your question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e451cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.output_len = 1000 ## we can modify on the fly if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7252af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can manually set the request\n",
    "## text = \"Explain Einstein's general theory of relativity\"\n",
    "## print(model.get_response(text))\n",
    "## or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use looped input\n",
    "text = model.get_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5768592c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75630b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
